{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_JSON = 'gdrive/MyDrive/DSA4262/data.json' # Path to data.json\n",
    "DATA_INFO = 'gdrive/MyDrive/DSA4262/data.info' # Path to data.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df = pd.read_csv(DATA_INFO)\n",
    "labels_dict = {}\n",
    "\n",
    "for _, row in labels_df.iterrows():\n",
    "  labels_dict[row['transcript_id']] = labels_dict.get(row['transcript_id'], {})\n",
    "  labels_dict[row['transcript_id']][row['transcript_position']] = row['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_gene_id = list(labels_df.gene_id.unique())\n",
    "\n",
    "unique_gene_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split gene ID for training and testing\n",
    "\n",
    "n = len(unique_gene_id)\n",
    "test_gene_id = unique_gene_id[:int(0.2 * len(unique_gene_id))]\n",
    "train_gene_id = unique_gene_id[int(0.2 * len(unique_gene_id)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_df = labels_df[labels_df['gene_id'].isin(train_gene_id)]\n",
    "test_labels_df = labels_df[labels_df['gene_id'].isin(test_gene_id)]\n",
    "\n",
    "train0, train1 = train_labels_df[\"label\"].value_counts()\n",
    "test0, test1 = test_labels_df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Train class ratio {int(train0/train1)}:1') # 21:1\n",
    "print(f'Test class ratio {int(test0/test1)}:1') # 20:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_lists = []\n",
    "\n",
    "with open(DATA_JSON) as f:\n",
    "  for transcript_json in f:\n",
    "    transcript_dict = json.loads(transcript_json)\n",
    "    for transcript_id, transcript_pos_dict in transcript_dict.items():\n",
    "      for transcript_pos, nucleotides_dict in transcript_pos_dict.items():\n",
    "        for nucleotides, data in nucleotides_dict.items():\n",
    "          for row in data:\n",
    "            instance_lists.append([transcript_id, transcript_pos, nucleotides] + row + [labels_dict[transcript_id][int(transcript_pos)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df = pd.DataFrame(instance_lists, columns=['transcript_id', 'transcript_position', 'nucleotides', '0', '1', '2', '3', '4', '5', '6', '7', '8','label'])\n",
    "\n",
    "complete_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df_mean = complete_df.groupby(by=['transcript_id', 'transcript_position', 'nucleotides']).mean().reset_index()\n",
    "\n",
    "complete_df_mean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df_min = complete_df.groupby(by=['transcript_id', 'transcript_position', 'nucleotides']).min().reset_index()\n",
    "complete_df_min.columns = ['transcript_id', 'transcript_position', 'nucleotides', '9', '10', '11', '12', '13', '14', '15', '16', '17', 'label']\n",
    "\n",
    "complete_df_min.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df_max = complete_df.groupby(by=['transcript_id', 'transcript_position', 'nucleotides']).max().reset_index()\n",
    "complete_df_max.columns = ['transcript_id', 'transcript_position', 'nucleotides', '18', '19', '20', '21', '22', '23', '24', '25', '26', 'label']\n",
    "\n",
    "complete_df_max.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df_all = complete_df_mean.drop(columns=['label']).merge(complete_df_min.drop(columns=['label'])).merge(complete_df_max.drop(columns=['label']))\n",
    "complete_df_all['transcript_position'] = complete_df_all['transcript_position'].astype('int')\n",
    "\n",
    "complete_df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df_all = complete_df_all.merge(labels_df, on=['transcript_id', 'transcript_position'])\n",
    "\n",
    "complete_df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = labels_df.label.value_counts()\n",
    "label_ratio = label_counts[0]/label_counts[1]\n",
    "\n",
    "label_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save features and labels dataframes to csv\n",
    "\n",
    "complete_df_all.iloc[:, 3:-2].to_csv('gdrive/MyDrive/DSA4262/xgb3_feature.csv', index=False)\n",
    "complete_df_all.iloc[:, -1].to_csv('gdrive/MyDrive/DSA4262/xgb3_label.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, roc_curve, precision_recall_curve, auc\n",
    "from scipy.stats import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_roc_auc(y_true, y_pred):\n",
    "    fpr, tpr, _  = roc_curve(y_true, y_pred)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    return roc_auc\n",
    "\n",
    "\n",
    "def get_pr_auc(y_true, y_pred):\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_pred, pos_label=1)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    return pr_auc\n",
    "\n",
    "\n",
    "def get_accuracy(y_true, y_pred):\n",
    "    return balanced_accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost==1.6.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make sure our model is robust against unseen data, we perform multiple iterations of model training and evaluation\n",
    "# Each time, we split the data into training and evaluation set by gene ID\n",
    "\n",
    "roc_auc = []\n",
    "pr_auc = []\n",
    "accuracy = []\n",
    "\n",
    "for i in range(len(unique_gene_id)):\n",
    "  print(f'Inspecting gene {i+1}/{len(unique_gene_id)}')\n",
    "  gene_id = unique_gene_id[i]\n",
    "  train_df = complete_df_all[complete_df_all['gene_id'] != gene_id].iloc[:, 3:-2]\n",
    "  train_label = complete_df_all[complete_df_all['gene_id'] != gene_id].iloc[:, -1]\n",
    "  eval_df = complete_df_all[complete_df_all['gene_id'] != gene_id].iloc[:, 3:-2]\n",
    "  eval_label = complete_df_all[complete_df_all['gene_id'] != gene_id].iloc[:, -1]\n",
    "  \n",
    "  xgb_model = XGBClassifier(\n",
    "      objective = 'binary:logistic',\n",
    "      scale_pos_weight = ceil(label_ratio),\n",
    "      max_delta_step = 1,\n",
    "      seed = 0,\n",
    "      tree_method = 'gpu_hist'\n",
    "  )\n",
    "\n",
    "  xgb_model.fit(train_df, train_label)\n",
    "  predictions = xgb_model.predict(eval_df)\n",
    "\n",
    "  roc_auc.append(get_roc_auc(eval_label.to_numpy(), predictions))\n",
    "  pr_auc.append(get_pr_auc(eval_label.to_numpy(), predictions))\n",
    "  accuracy.append(get_accuracy(eval_label.to_numpy(), predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'ROC AUC: {np.mean(roc_auc)}') # ROC AUC: 0.9392459265323052\n",
    "print(f'PR AUC: {np.mean(pr_auc)}') # PR AUC: 0.6521694199992294\n",
    "print(f'Accuracy: {np.mean(accuracy)}') # Accuracy: 0.9392459265323052"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same thing now, but we use mean only to compare the performances of the two models\n",
    "\n",
    "complete_df_mean['transcript_position'] = complete_df_mean['transcript_position'].astype('int')\n",
    "complete_df_mean = complete_df_mean.drop(columns=['label']).merge(labels_df, on=['transcript_id', 'transcript_position'])\n",
    "\n",
    "complete_df_mean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean only\n",
    "\n",
    "roc_auc = []\n",
    "pr_auc = []\n",
    "accuracy = []\n",
    "\n",
    "for i in range(len(unique_gene_id)):\n",
    "  print(f'Inspecting gene {i+1}/{len(unique_gene_id)}')\n",
    "  gene_id = unique_gene_id[i]\n",
    "  train_df = complete_df_mean[complete_df_mean['gene_id'] != gene_id].iloc[:, 3:-2]\n",
    "  train_label = complete_df_mean[complete_df_mean['gene_id'] != gene_id].iloc[:, -1]\n",
    "  eval_df = complete_df_mean[complete_df_mean['gene_id'] != gene_id].iloc[:, 3:-2]\n",
    "  eval_label = complete_df_mean[complete_df_mean['gene_id'] != gene_id].iloc[:, -1]\n",
    "  \n",
    "  xgb_model = XGBClassifier(\n",
    "      objective = 'binary:logistic',\n",
    "      scale_pos_weight = ceil(label_ratio),\n",
    "      max_delta_step = 1,\n",
    "      seed = 0,\n",
    "      tree_method = 'gpu_hist'\n",
    "  )\n",
    "\n",
    "  xgb_model.fit(train_df, train_label)\n",
    "  predictions = xgb_model.predict(eval_df)\n",
    "\n",
    "  roc_auc.append(get_roc_auc(eval_label.to_numpy(), predictions))\n",
    "  pr_auc.append(get_pr_auc(eval_label.to_numpy(), predictions))\n",
    "  accuracy.append(get_accuracy(eval_label.to_numpy(), predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'ROC AUC: {np.mean(roc_auc)}') # ROC AUC: 0.913483812474404\n",
    "print(f'ROC AUC: {np.mean(pr_auc)}') # PR AUC: 0.6115034727496196\n",
    "print(f'ROC AUC: {np.mean(accuracy)}') # Accuracy: 0.913483812474404"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model with the full dataset and save it\n",
    "\n",
    "xgb_model = XGBClassifier(\n",
    "    objective = 'binary:logistic',\n",
    "    scale_pos_weight = ceil(label_ratio),\n",
    "    max_delta_step = 1,\n",
    "    seed = 0,\n",
    "    tree_method = 'gpu_hist'\n",
    ")\n",
    "\n",
    "xgb_model.fit(complete_df_all.iloc[:, 3:-2], complete_df_all.iloc[:, -1])\n",
    "predictions = xgb_model.predict(complete_df_all.iloc[:, 3:-2])\n",
    "\n",
    "print(f'ROC AUC: {get_roc_auc(complete_df_all.iloc[:, -1].to_numpy(), predictions)}')\n",
    "print(f'PR AUC: {get_pr_auc(complete_df_all.iloc[:, -1].to_numpy(), predictions)}')\n",
    "print(f'Accuracy: {get_accuracy(complete_df_all.iloc[:, -1].to_numpy(), predictions)}')\n",
    "\n",
    "xgb_model.save_model('gdrive/MyDrive/DSA4262/xgb6.model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit ('3.9.12')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "06896bca7bf1bedf7afe52dc08257873da07984ca27f948886f822b7d0010424"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
